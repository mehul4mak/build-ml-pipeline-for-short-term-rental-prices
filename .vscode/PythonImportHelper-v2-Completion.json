[
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "wandb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wandb",
        "description": "wandb",
        "detail": "wandb",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "scipy.stats",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "log_artifact",
        "importPath": "wandb_utils.log_artifact",
        "description": "wandb_utils.log_artifact",
        "isExtraImport": true,
        "detail": "wandb_utils.log_artifact",
        "documentation": {}
    },
    {
        "label": "log_artifact",
        "importPath": "wandb_utils.log_artifact",
        "description": "wandb_utils.log_artifact",
        "isExtraImport": true,
        "detail": "wandb_utils.log_artifact",
        "documentation": {}
    },
    {
        "label": "log_artifact",
        "importPath": "wandb_utils.log_artifact",
        "description": "wandb_utils.log_artifact",
        "isExtraImport": true,
        "detail": "wandb_utils.log_artifact",
        "documentation": {}
    },
    {
        "label": "mlflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mlflow",
        "description": "mlflow",
        "detail": "mlflow",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_absolute_error",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "train_test_split",
        "importPath": "sklearn.model_selection",
        "description": "sklearn.model_selection",
        "isExtraImport": true,
        "detail": "sklearn.model_selection",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "ColumnTransformer",
        "importPath": "sklearn.compose",
        "description": "sklearn.compose",
        "isExtraImport": true,
        "detail": "sklearn.compose",
        "documentation": {}
    },
    {
        "label": "TfidfVectorizer",
        "importPath": "sklearn.feature_extraction.text",
        "description": "sklearn.feature_extraction.text",
        "isExtraImport": true,
        "detail": "sklearn.feature_extraction.text",
        "documentation": {}
    },
    {
        "label": "SimpleImputer",
        "importPath": "sklearn.impute",
        "description": "sklearn.impute",
        "isExtraImport": true,
        "detail": "sklearn.impute",
        "documentation": {}
    },
    {
        "label": "OrdinalEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "OneHotEncoder",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "FunctionTransformer",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "RandomForestRegressor",
        "importPath": "sklearn.ensemble",
        "description": "sklearn.ensemble",
        "isExtraImport": true,
        "detail": "sklearn.ensemble",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "make_pipeline",
        "importPath": "sklearn.pipeline",
        "description": "sklearn.pipeline",
        "isExtraImport": true,
        "detail": "sklearn.pipeline",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "imagehash",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imagehash",
        "description": "imagehash",
        "detail": "imagehash",
        "documentation": {}
    },
    {
        "label": "hydra",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hydra",
        "description": "hydra",
        "detail": "hydra",
        "documentation": {}
    },
    {
        "label": "DictConfig",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "pytest_addoption",
        "kind": 2,
        "importPath": "components.data_check.conftest",
        "description": "components.data_check.conftest",
        "peekOfCode": "def pytest_addoption(parser):\n    parser.addoption(\"--csv\", action=\"store\")\n    parser.addoption(\"--ref\", action=\"store\")\n    parser.addoption(\"--kl_threshold\", action=\"store\")\n    parser.addoption(\"--min_price\", action=\"store\")\n    parser.addoption(\"--max_price\", action=\"store\")\n@pytest.fixture(scope='session')\ndef data(request):\n    run = wandb.init(job_type=\"data_tests\",resume=True)\n    # Download input artifact. This will also note that this script is using this",
        "detail": "components.data_check.conftest",
        "documentation": {}
    },
    {
        "label": "data",
        "kind": 2,
        "importPath": "components.data_check.conftest",
        "description": "components.data_check.conftest",
        "peekOfCode": "def data(request):\n    run = wandb.init(job_type=\"data_tests\",resume=True)\n    # Download input artifact. This will also note that this script is using this\n    # particular version of the artifact\n    data_path = run.use_artifact(request.config.option.csv).file()\n    if data_path is None:\n        pytest.fail(\"You must provide the --csv option on the command line\")\n    df = pd.read_csv(data_path)\n    return df\n@pytest.fixture(scope='session')",
        "detail": "components.data_check.conftest",
        "documentation": {}
    },
    {
        "label": "ref_data",
        "kind": 2,
        "importPath": "components.data_check.conftest",
        "description": "components.data_check.conftest",
        "peekOfCode": "def ref_data(request):\n    run = wandb.init(job_type=\"data_tests\",resume=True)\n    # Download input artifact. This will also note that this script is using this\n    # particular version of the artifact\n    data_path = run.use_artifact(request.config.option.ref).file()\n    if data_path is None:\n        pytest.fail(\"You must provide the --ref option on the command line\")\n    df = pd.read_csv(data_path)\n    return df\n@pytest.fixture(scope='session')",
        "detail": "components.data_check.conftest",
        "documentation": {}
    },
    {
        "label": "kl_threshold",
        "kind": 2,
        "importPath": "components.data_check.conftest",
        "description": "components.data_check.conftest",
        "peekOfCode": "def kl_threshold(request):\n    kl_threshold = request.config.option.kl_threshold\n    if kl_threshold is None:\n        pytest.fail(\"You must provide a threshold for the KL test\")\n    return float(kl_threshold)\n@pytest.fixture(scope='session')\ndef min_price(request):\n    min_price = request.config.option.min_price\n    if min_price is None:\n        pytest.fail(\"You must provide min_price\")",
        "detail": "components.data_check.conftest",
        "documentation": {}
    },
    {
        "label": "min_price",
        "kind": 2,
        "importPath": "components.data_check.conftest",
        "description": "components.data_check.conftest",
        "peekOfCode": "def min_price(request):\n    min_price = request.config.option.min_price\n    if min_price is None:\n        pytest.fail(\"You must provide min_price\")\n    return float(min_price)\n@pytest.fixture(scope='session')\ndef max_price(request):\n    max_price = request.config.option.max_price\n    if max_price is None:\n        pytest.fail(\"You must provide max_price\")",
        "detail": "components.data_check.conftest",
        "documentation": {}
    },
    {
        "label": "max_price",
        "kind": 2,
        "importPath": "components.data_check.conftest",
        "description": "components.data_check.conftest",
        "peekOfCode": "def max_price(request):\n    max_price = request.config.option.max_price\n    if max_price is None:\n        pytest.fail(\"You must provide max_price\")\n    return float(max_price)",
        "detail": "components.data_check.conftest",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 5,
        "importPath": "components.data_check.conftest",
        "description": "components.data_check.conftest",
        "peekOfCode": "run = wandb.init(job_type=\"data_tests\")\ndef pytest_addoption(parser):\n    parser.addoption(\"--csv\", action=\"store\")\n    parser.addoption(\"--ref\", action=\"store\")\n    parser.addoption(\"--kl_threshold\", action=\"store\")\n    parser.addoption(\"--min_price\", action=\"store\")\n    parser.addoption(\"--max_price\", action=\"store\")\n@pytest.fixture(scope='session')\ndef data(request):\n    run = wandb.init(job_type=\"data_tests\",resume=True)",
        "detail": "components.data_check.conftest",
        "documentation": {}
    },
    {
        "label": "test_column_names",
        "kind": 2,
        "importPath": "components.data_check.test_data",
        "description": "components.data_check.test_data",
        "peekOfCode": "def test_column_names(data):\n    expected_colums = [\n        \"id\",\n        \"name\",\n        \"host_id\",\n        \"host_name\",\n        \"neighbourhood_group\",\n        \"neighbourhood\",\n        \"latitude\",\n        \"longitude\",",
        "detail": "components.data_check.test_data",
        "documentation": {}
    },
    {
        "label": "test_neighborhood_names",
        "kind": 2,
        "importPath": "components.data_check.test_data",
        "description": "components.data_check.test_data",
        "peekOfCode": "def test_neighborhood_names(data):\n    known_names = [\"Bronx\", \"Brooklyn\", \"Manhattan\", \"Queens\", \"Staten Island\"]\n    neigh = set(data['neighbourhood_group'].unique())\n    # Unordered check\n    assert set(known_names) == set(neigh)\ndef test_proper_boundaries(data: pd.DataFrame):\n    \"\"\"\n    Test proper longitude and latitude boundaries for properties in and around NYC\n    \"\"\"\n    idx = data['longitude'].between(-74.25, - \\",
        "detail": "components.data_check.test_data",
        "documentation": {}
    },
    {
        "label": "test_proper_boundaries",
        "kind": 2,
        "importPath": "components.data_check.test_data",
        "description": "components.data_check.test_data",
        "peekOfCode": "def test_proper_boundaries(data: pd.DataFrame):\n    \"\"\"\n    Test proper longitude and latitude boundaries for properties in and around NYC\n    \"\"\"\n    idx = data['longitude'].between(-74.25, - \\\n                                    73.50) & data['latitude'].between(40.5, 41.2)\n    assert np.sum(~idx) == 0\ndef test_similar_neigh_distrib(\n        data: pd.DataFrame,\n        ref_data: pd.DataFrame,",
        "detail": "components.data_check.test_data",
        "documentation": {}
    },
    {
        "label": "test_similar_neigh_distrib",
        "kind": 2,
        "importPath": "components.data_check.test_data",
        "description": "components.data_check.test_data",
        "peekOfCode": "def test_similar_neigh_distrib(\n        data: pd.DataFrame,\n        ref_data: pd.DataFrame,\n        kl_threshold: float):\n    \"\"\"\n    Apply a threshold on the KL divergence to detect if the distribution of the new data is\n    significantly different than that of the reference dataset\n    \"\"\"\n    dist1 = data['neighbourhood_group'].value_counts().sort_index()\n    dist2 = ref_data['neighbourhood_group'].value_counts().sort_index()",
        "detail": "components.data_check.test_data",
        "documentation": {}
    },
    {
        "label": "test_row_count",
        "kind": 2,
        "importPath": "components.data_check.test_data",
        "description": "components.data_check.test_data",
        "peekOfCode": "def test_row_count(data: pd.DataFrame) -> None:\n    \"\"\"test_row_count in given range for input dataframe\n    Args:\n        data (pd.DataFrame): input dataframe\n    \"\"\"\n    assert 15000 < data.shape[0] < 1000000\ndef test_price_range(data: pd.DataFrame, min_price: float, max_price: float):\n    \"\"\"test_price_range\n    Args:\n        data (pd.DataFrame): input dataframe",
        "detail": "components.data_check.test_data",
        "documentation": {}
    },
    {
        "label": "test_price_range",
        "kind": 2,
        "importPath": "components.data_check.test_data",
        "description": "components.data_check.test_data",
        "peekOfCode": "def test_price_range(data: pd.DataFrame, min_price: float, max_price: float):\n    \"\"\"test_price_range\n    Args:\n        data (pd.DataFrame): input dataframe\n        min_price (float): minimum price value \n        max_price (float): maximum price value\n    \"\"\"\n    assert data['price'].between(min_price, max_price)",
        "detail": "components.data_check.test_data",
        "documentation": {}
    },
    {
        "label": "go",
        "kind": 2,
        "importPath": "components.get_data.run",
        "description": "components.get_data.run",
        "peekOfCode": "def go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n    logger.info(f\"Returning sample {args.sample}\")\n    logger.info(f\"Uploading {args.artifact_name} to Weights & Biases\")\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,\n        os.path.join(\"data\", args.sample),",
        "detail": "components.get_data.run",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "components.get_data.run",
        "description": "components.get_data.run",
        "peekOfCode": "logger = logging.getLogger()\ndef go(args):\n    run = wandb.init(job_type=\"download_file\")\n    run.config.update(args)\n    logger.info(f\"Returning sample {args.sample}\")\n    logger.info(f\"Uploading {args.artifact_name} to Weights & Biases\")\n    log_artifact(\n        args.artifact_name,\n        args.artifact_type,\n        args.artifact_description,",
        "detail": "components.get_data.run",
        "documentation": {}
    },
    {
        "label": "go",
        "kind": 2,
        "importPath": "components.test_regression_model.run",
        "description": "components.test_regression_model.run",
        "peekOfCode": "def go(args):\n    run = wandb.init(job_type=\"test_model\")\n    run.config.update(args)\n    logger.info(\"Downloading artifacts\")\n    # Download input artifact. This will also log that this script is using this\n    # particular version of the artifact\n    model_local_path = run.use_artifact(args.mlflow_model).download()\n    # Download test dataset\n    test_dataset_path = run.use_artifact(args.test_dataset).file()\n    # Read test dataset",
        "detail": "components.test_regression_model.run",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "components.test_regression_model.run",
        "description": "components.test_regression_model.run",
        "peekOfCode": "logger = logging.getLogger()\ndef go(args):\n    run = wandb.init(job_type=\"test_model\")\n    run.config.update(args)\n    logger.info(\"Downloading artifacts\")\n    # Download input artifact. This will also log that this script is using this\n    # particular version of the artifact\n    model_local_path = run.use_artifact(args.mlflow_model).download()\n    # Download test dataset\n    test_dataset_path = run.use_artifact(args.test_dataset).file()",
        "detail": "components.test_regression_model.run",
        "documentation": {}
    },
    {
        "label": "go",
        "kind": 2,
        "importPath": "components.train_val_test_split.run",
        "description": "components.train_val_test_split.run",
        "peekOfCode": "def go(args):\n    run = wandb.init(job_type=\"train_val_test_split\")\n    run.config.update(args)\n    # Download input artifact. This will also note that this script is using this\n    # particular version of the artifact\n    logger.info(f\"Fetching artifact {args.input}\")\n    artifact_local_path = run.use_artifact(args.input).file()\n    df = pd.read_csv(artifact_local_path)\n    logger.info(\"Splitting trainval and test\")\n    trainval, test = train_test_split(",
        "detail": "components.train_val_test_split.run",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "components.train_val_test_split.run",
        "description": "components.train_val_test_split.run",
        "peekOfCode": "logger = logging.getLogger()\ndef go(args):\n    run = wandb.init(job_type=\"train_val_test_split\")\n    run.config.update(args)\n    # Download input artifact. This will also note that this script is using this\n    # particular version of the artifact\n    logger.info(f\"Fetching artifact {args.input}\")\n    artifact_local_path = run.use_artifact(args.input).file()\n    df = pd.read_csv(artifact_local_path)\n    logger.info(\"Splitting trainval and test\")",
        "detail": "components.train_val_test_split.run",
        "documentation": {}
    },
    {
        "label": "log_artifact",
        "kind": 2,
        "importPath": "components.wandb_utils.log_artifact",
        "description": "components.wandb_utils.log_artifact",
        "peekOfCode": "def log_artifact(artifact_name, artifact_type, artifact_description, filename, wandb_run):\n    \"\"\"\n    Log the provided filename as an artifact in W&B, and add the artifact path to the MLFlow run\n    so it can be retrieved by subsequent steps in a pipeline\n    :param artifact_name: name for the artifact\n    :param artifact_type: type for the artifact (just a string like \"raw_data\", \"clean_data\" and so on)\n    :param artifact_description: a brief description of the artifact\n    :param filename: local filename for the artifact\n    :param wandb_run: current Weights & Biases run\n    :return: None",
        "detail": "components.wandb_utils.log_artifact",
        "documentation": {}
    },
    {
        "label": "sanitize_path",
        "kind": 2,
        "importPath": "components.wandb_utils.sanitize_path",
        "description": "components.wandb_utils.sanitize_path",
        "peekOfCode": "def sanitize_path(s):\n    \"\"\"\n    Sanitizes the input path by:\n    1. Expanding environment variables\n    2. Expanding the home directory ('~')\n    3. Calculating the absolute path\n    :param s: input path\n    :return: a sanitized version of the input path\n    \"\"\"\n    return os.path.abspath(os.path.expanduser(os.path.expandvars(s)))",
        "detail": "components.wandb_utils.sanitize_path",
        "documentation": {}
    },
    {
        "label": "go",
        "kind": 2,
        "importPath": "src.basic_cleaning.run",
        "description": "src.basic_cleaning.run",
        "peekOfCode": "def go(args):\n    with wandb.init(job_type=\"basic_cleaning\") as run:\n        logger.info(\"Creating wandb run...\")\n        run.config.update(args)\n        logger.info(\"Downloading raw artifact\")\n        # run = wandb.init(project=\"nyc_airbnb\", group=\"eda\", save_code=True)\n        artifact_local_path = run.use_artifact(args.input_artifact).file()\n        df = pd.read_csv(artifact_local_path)\n        logger.info(\"Creating cleaned artifact\")\n        # Drop outliers",
        "detail": "src.basic_cleaning.run",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "src.basic_cleaning.run",
        "description": "src.basic_cleaning.run",
        "peekOfCode": "logger = logging.getLogger()\ndef go(args):\n    with wandb.init(job_type=\"basic_cleaning\") as run:\n        logger.info(\"Creating wandb run...\")\n        run.config.update(args)\n        logger.info(\"Downloading raw artifact\")\n        # run = wandb.init(project=\"nyc_airbnb\", group=\"eda\", save_code=True)\n        artifact_local_path = run.use_artifact(args.input_artifact).file()\n        df = pd.read_csv(artifact_local_path)\n        logger.info(\"Creating cleaned artifact\")",
        "detail": "src.basic_cleaning.run",
        "documentation": {}
    },
    {
        "label": "delta_date_feature",
        "kind": 2,
        "importPath": "src.train_random_forest.feature_engineering",
        "description": "src.train_random_forest.feature_engineering",
        "peekOfCode": "def delta_date_feature(dates):\n    \"\"\"\n    Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days\n    between each date and the most recent date in its column\n    \"\"\"\n    date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)\n    return date_sanitized.apply(\n        lambda d: (\n            d.max() - d).dt.days,\n        axis=0).to_numpy()",
        "detail": "src.train_random_forest.feature_engineering",
        "documentation": {}
    },
    {
        "label": "delta_date_feature",
        "kind": 2,
        "importPath": "src.train_random_forest.run",
        "description": "src.train_random_forest.run",
        "peekOfCode": "def delta_date_feature(dates):\n    \"\"\"\n    Given a 2d array containing dates (in any format recognized by pd.to_datetime), it returns the delta in days\n    between each date and the most recent date in its column\n    \"\"\"\n    date_sanitized = pd.DataFrame(dates).apply(pd.to_datetime)\n    return date_sanitized.apply(\n        lambda d: (\n            d.max() - d).dt.days,\n        axis=0).to_numpy()",
        "detail": "src.train_random_forest.run",
        "documentation": {}
    },
    {
        "label": "go",
        "kind": 2,
        "importPath": "src.train_random_forest.run",
        "description": "src.train_random_forest.run",
        "peekOfCode": "def go(args):\n    run = wandb.init(job_type=\"train_random_forest\")\n    run.config.update(args)\n    # Get the Random Forest configuration and update W&B\n    with open(args.rf_config) as fp:\n        rf_config = json.load(fp)\n    run.config.update(rf_config)\n    # Fix the random seed for the Random Forest, so we get reproducible results\n    rf_config['random_state'] = args.random_seed\n    ######################################",
        "detail": "src.train_random_forest.run",
        "documentation": {}
    },
    {
        "label": "plot_feature_importance",
        "kind": 2,
        "importPath": "src.train_random_forest.run",
        "description": "src.train_random_forest.run",
        "peekOfCode": "def plot_feature_importance(pipe, feat_names):\n    # We collect the feature importance for all non-nlp features first\n    feat_imp = pipe[\"random_forest\"].feature_importances_[\n        : len(feat_names) - 1]\n    # For the NLP feature we sum across all the TF-IDF dimensions into a global\n    # NLP importance\n    nlp_importance = sum(\n        pipe[\"random_forest\"].feature_importances_[\n            len(feat_names) - 1:])\n    feat_imp = np.append(feat_imp, nlp_importance)",
        "detail": "src.train_random_forest.run",
        "documentation": {}
    },
    {
        "label": "get_inference_pipeline",
        "kind": 2,
        "importPath": "src.train_random_forest.run",
        "description": "src.train_random_forest.run",
        "peekOfCode": "def get_inference_pipeline(rf_config, max_tfidf_features):\n    # Let's handle the categorical features first\n    # Ordinal categorical are categorical values for which the order is meaningful, for example\n    # for room type: 'Entire home/apt' > 'Private room' > 'Shared room'\n    ordinal_categorical = [\"room_type\"]\n    non_ordinal_categorical = [\"neighbourhood_group\"]\n    # NOTE: we do not need to impute room_type because the type of the room\n    # is mandatory on the websites, so missing values are not possible in production\n    # (nor during training). That is not true for neighbourhood_group\n    ordinal_categorical_preproc = OrdinalEncoder()",
        "detail": "src.train_random_forest.run",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "src.train_random_forest.run",
        "description": "src.train_random_forest.run",
        "peekOfCode": "logger = logging.getLogger()\ndef go(args):\n    run = wandb.init(job_type=\"train_random_forest\")\n    run.config.update(args)\n    # Get the Random Forest configuration and update W&B\n    with open(args.rf_config) as fp:\n        rf_config = json.load(fp)\n    run.config.update(rf_config)\n    # Fix the random seed for the Random Forest, so we get reproducible results\n    rf_config['random_state'] = args.random_seed",
        "detail": "src.train_random_forest.run",
        "documentation": {}
    },
    {
        "label": "find_similar_images",
        "kind": 2,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "def find_similar_images(userpaths, hashfunc=imagehash.average_hash):\n\tdef is_image(filename):\n\t\tf = filename.lower()\n\t\treturn f.endswith('.png') or f.endswith('.jpg') or \\\n\t\t\tf.endswith('.jpeg') or f.endswith('.bmp') or \\\n\t\t\tf.endswith('.gif') or '.jpg' in f or f.endswith('.svg')\n\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\tf",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\tf = filename.lower()\n\t\treturn f.endswith('.png') or f.endswith('.jpg') or \\\n\t\t\tf.endswith('.jpeg') or f.endswith('.bmp') or \\\n\t\t\tf.endswith('.gif') or '.jpg' in f or f.endswith('.svg')\n\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\timage_filenames",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\timage_filenames = []\n\tfor userpath in userpaths:\n\t\timage_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:\n\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\timages",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\timages = {}\n\tfor img in sorted(image_filenames):\n\t\ttry:\n\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue\n\t\tif hash in images:\n\t\t\tprint(img, '  already exists as', ' '.join(images[hash]))\n\t\t\tif 'dupPictures' in img:",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\t\thash",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\t\thash = hashfunc(Image.open(img))\n\t\texcept Exception as e:\n\t\t\tprint('Problem:', e, 'with', img)\n\t\t\tcontinue\n\t\tif hash in images:\n\t\t\tprint(img, '  already exists as', ' '.join(images[hash]))\n\t\t\tif 'dupPictures' in img:\n\t\t\t\tprint('rm -v', img)\n\t\timages[hash] = images.get(hash, []) + [img]\n\t# for k, img_list in six.iteritems(images):",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\timages[hash]",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\timages[hash] = images.get(hash, []) + [img]\n\t# for k, img_list in six.iteritems(images):\n\t# \tif len(img_list) > 1:\n\t# \t\tprint(\" \".join(img_list))\nif __name__ == '__main__':  # noqa: C901\n\timport os\n\timport sys\n\tdef usage():\n\t\tsys.stderr.write(\"\"\"SYNOPSIS: %s [ahash|phash|dhash|...] [<directory>]\nIdentifies similar images in the directory.",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\thashmethod",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\thashmethod = sys.argv[1] if len(sys.argv) > 1 else usage()\n\tif hashmethod == 'ahash':\n\t\thashfunc = imagehash.average_hash\n\telif hashmethod == 'phash':\n\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.average_hash\n\telif hashmethod == 'phash':\n\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.phash\n\telif hashmethod == 'dhash':\n\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.dhash\n\telif hashmethod == 'whash-haar':\n\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.whash\n\telif hashmethod == 'whash-db4':\n\t\tdef hashfunc(img):\n\t\t\treturn imagehash.whash(img, mode='db4')\n\telif hashmethod == 'colorhash':\n\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.colorhash\n\telif hashmethod == 'crop-resistant':\n\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()\n\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\t\thashfunc",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\t\thashfunc = imagehash.crop_resistant_hash\n\telse:\n\t\tusage()\n\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "\tuserpaths",
        "kind": 5,
        "importPath": "venv.bin.find_similar_images",
        "description": "venv.bin.find_similar_images",
        "peekOfCode": "\tuserpaths = sys.argv[2:] if len(sys.argv) > 2 else '.'\n\tfind_similar_images(userpaths=userpaths, hashfunc=hashfunc)",
        "detail": "venv.bin.find_similar_images",
        "documentation": {}
    },
    {
        "label": "go",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def go(config: DictConfig):\n    # Setup the wandb experiment. All runs will be grouped under this name\n    os.environ[\"WANDB_PROJECT\"] = config[\"main\"][\"project_name\"]\n    os.environ[\"WANDB_RUN_GROUP\"] = config[\"main\"][\"experiment_name\"]\n    # Steps to execute\n    steps_par = config['main']['steps']\n    active_steps = steps_par.split(\",\") if steps_par != \"all\" else _steps\n    # Move to a temporary directory\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        if \"download\" in active_steps:",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "_steps",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "_steps = [\n    \"download\",\n    \"basic_cleaning\",\n    \"data_check\",\n    \"data_split\",\n    \"train_random_forest\",\n    # NOTE: We do not include this in the steps so it is not run by mistake.\n    # You first need to promote a model export to \"prod\" before you can run this,\n    # then you need to run this step explicitly\n    #    \"test_regression_model\"",
        "detail": "main",
        "documentation": {}
    }
]